name: Weekly Parquet Build

on:
  schedule:
    # Run at 8 AM UTC every Sunday
    - cron: '0 8 * * 0'
  workflow_dispatch:
    inputs:
      full_rebuild:
        description: 'Force full database rebuild'
        type: boolean
        default: false

jobs:
  build-parquets:
    runs-on: ubuntu-latest
    timeout-minutes: 180  # 3 hour timeout for large builds
    permissions:
      contents: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Setup R
        uses: r-lib/actions/setup-r@v2
        with:
          r-version: '4.4.0'

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y libcurl4-openssl-dev

      - name: Install R dependencies
        run: |
          install.packages(c(
            "duckdb",
            "arrow",
            "jsonlite",
            "httr2",
            "zip",
            "cli"
          ), repos = "https://cloud.r-project.org")
        shell: Rscript {0}

      - name: Download JSON archives from latest release
        run: |
          source("scripts/download_json_archives.R")
        shell: Rscript {0}
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Build DuckDB database
        run: |
          source("scripts/build_database.R")
        shell: Rscript {0}

      - name: Export tables to Parquet
        run: |
          source("scripts/export_parquets.R")
        shell: Rscript {0}

      - name: Get release info
        id: release_info
        run: |
          WEEK_NUM=$(date +%V)
          YEAR=$(date +%Y)
          echo "RELEASE_TAG=v${YEAR}.W${WEEK_NUM}" >> $GITHUB_ENV
          echo "RELEASE_NAME=Weekly Build - ${YEAR} Week ${WEEK_NUM}" >> $GITHUB_ENV

      - name: Create Weekly Release
        uses: softprops/action-gh-release@v1
        with:
          tag_name: ${{ env.RELEASE_TAG }}-weekly
          name: ${{ env.RELEASE_NAME }}
          body: |
            ## Weekly Parquet Build

            **Week**: ${{ env.RELEASE_TAG }}

            ### Parquet Files
            These files can be queried directly with DuckDB, Arrow, or pandas.

            #### Core Tables
            - `matches.parquet` - Match metadata
            - `deliveries_long_form.parquet` - Ball-by-ball data (Tests, First-class)
            - `deliveries_short_form.parquet` - Ball-by-ball data (ODIs, T20s, etc.)
            - `players.parquet` - Player registry

            #### Skill Index Tables
            - `test_player_skill.parquet`
            - `odi_player_skill.parquet`
            - `t20_player_skill.parquet`
            - `test_team_skill.parquet`
            - `odi_team_skill.parquet`
            - `t20_team_skill.parquet`
            - `test_venue_skill.parquet`
            - `odi_venue_skill.parquet`
            - `t20_venue_skill.parquet`

            #### Other Tables
            - `team_elo.parquet` - Team ELO ratings

            ### Usage
            ```r
            # Query directly with DuckDB
            library(duckdb)
            con <- dbConnect(duckdb())
            matches <- dbGetQuery(con, "SELECT * FROM 'matches.parquet' LIMIT 10")

            # Or use Arrow
            library(arrow)
            matches <- read_parquet("matches.parquet")
            ```

            ```python
            # Python with pandas
            import pandas as pd
            matches = pd.read_parquet("matches.parquet")
            ```
          files: |
            parquet_output/*.parquet
            parquet_output/manifest.json
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Cleanup
        if: always()
        run: |
          rm -rf json_temp/ parquet_output/ *.duckdb
