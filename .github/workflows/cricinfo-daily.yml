# Daily ESPN Cricinfo Data Scraping
#
# Scrapes ball-by-ball commentary + Hawkeye data from ESPN Cricinfo using
# Playwright + stealth in headed mode (via Xvfb virtual display).
#
# Per match, produces 3 tables:
#   {match_id}_balls.parquet   - Ball-by-ball with Hawkeye fields
#   {match_id}_match.parquet   - Match metadata (venue, toss, result, officials)
#   {match_id}_innings.parquet - Batting scorecards with player details
#
# Output: cricinfo/{format}_{gender}/{match_id}_{table}.parquet
#
# Schedule: Daily at 12 PM UTC (after international matches)
# Runtime: ~10-60 min depending on number of new matches
#
# On schedule: scrapes top 3 series per format from series_list.csv
# On manual trigger: accepts series IDs, format filter, max matches, force flag

name: Cricinfo Scrape

on:
  schedule:
    - cron: '0 12 * * *'
  workflow_dispatch:
    inputs:
      series_ids:
        description: 'Comma-separated series IDs (leave empty for auto from CSV)'
        required: false
        type: string
      format:
        description: 'Filter by format (t20i, odi, test, or empty for all)'
        required: false
        type: string
      max_matches:
        description: 'Max matches per series'
        required: false
        default: '50'
        type: string
      max_series:
        description: 'Max series per format (for scheduled/auto mode)'
        required: false
        default: '3'
        type: string
      force:
        description: 'Re-scrape existing matches'
        required: false
        default: false
        type: boolean

permissions:
  contents: write
  issues: write

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 120

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Cache pip packages
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-cricinfo-${{ hashFiles('scripts/requirements-cricinfo.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-cricinfo-

      - name: Install Python dependencies
        run: |
          pip install -r scripts/requirements-cricinfo.txt
          playwright install chromium --with-deps

      - name: Install Xvfb
        run: sudo apt-get install -y xvfb

      - name: Restore previous data from release
        run: |
          # Create all possible format_gender directories
          for dir in t20i_male t20i_female odi_male odi_female test_male test_female; do
            mkdir -p "cricinfo/$dir"
          done
          mkdir -p /tmp/cricinfo_dl
          # Download all assets to temp dir, then sort by directory prefix
          gh release download cricinfo-rich -D /tmp/cricinfo_dl --pattern "*.parquet" 2>/dev/null || true
          RESTORED=0
          for f in /tmp/cricinfo_dl/*.parquet; do
            [ -f "$f" ] || continue
            BASENAME=$(basename "$f")
            # Asset names are prefixed with dir: t20i_male__12345_balls.parquet
            MATCHED=false
            for dir in t20i_male t20i_female odi_male odi_female test_male test_female; do
              if echo "$BASENAME" | grep -q "^${dir}__"; then
                LOCALNAME=$(echo "$BASENAME" | sed "s/^${dir}__//")
                # Backwards compat: rename _rich.parquet -> _balls.parquet
                LOCALNAME=$(echo "$LOCALNAME" | sed "s/_rich\.parquet$/_balls.parquet/")
                mv "$f" "cricinfo/$dir/$LOCALNAME"
                RESTORED=$((RESTORED + 1))
                MATCHED=true
                break
              fi
            done
            # Backwards compat: old format-only prefixes (t20i__, odi__, test__)
            if [ "$MATCHED" = "false" ]; then
              # Handle fixtures.parquet (top-level, no prefix)
              if [ "$BASENAME" = "fixtures.parquet" ]; then
                mv "$f" "cricinfo/fixtures.parquet"
                RESTORED=$((RESTORED + 1))
                MATCHED=true
              fi
            fi
            # Backwards compat: old format-only prefixes (t20i__, odi__, test__)
            if [ "$MATCHED" = "false" ]; then
              for fmt in t20i odi test; do
                if echo "$BASENAME" | grep -q "^${fmt}__"; then
                  LOCALNAME=$(echo "$BASENAME" | sed "s/^${fmt}__//")
                  LOCALNAME=$(echo "$LOCALNAME" | sed "s/_rich\.parquet$/_balls.parquet/")
                  mv "$f" "cricinfo/${fmt}_male/$LOCALNAME"
                  RESTORED=$((RESTORED + 1))
                  break
                fi
              done
            fi
          done
          rm -rf /tmp/cricinfo_dl
          echo "Restored $RESTORED parquet files from previous runs"
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Discover new series
        continue-on-error: true
        run: |
          xvfb-run --auto-servernum --server-args="-screen 0 1280x900x24" \
            python scripts/discover_series.py --update --scan-parquets --cricinfo-dir cricinfo

      - name: Commit updated series list
        continue-on-error: true
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add scripts/series_list.csv
          git diff --cached --quiet || git commit -m "Auto-update series_list.csv with newly discovered series"
          git push
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Run scraper
        id: scrape
        run: |
          ARGS="--output-dir cricinfo --max-matches ${{ inputs.max_matches || '50' }} --max-series ${{ inputs.max_series || '3' }} --scan-parquets"

          # Add series filter (manual trigger only)
          SERIES_INPUT="${{ inputs.series_ids }}"
          if [ -n "$SERIES_INPUT" ]; then
            # Convert comma-separated to space-separated for argparse nargs, collapse whitespace
            SERIES_ARGS=$(echo "$SERIES_INPUT" | tr ',' ' ' | tr -s ' ')
            ARGS="$ARGS --series $SERIES_ARGS"
          fi

          # Add format filter
          FORMAT_INPUT="${{ inputs.format }}"
          if [ -n "$FORMAT_INPUT" ]; then
            ARGS="$ARGS --format $FORMAT_INPUT"
          fi

          # Add force flag
          if [ "${{ inputs.force }}" = "true" ]; then
            ARGS="$ARGS --force"
          fi

          echo "Running: python scripts/cricinfo_rich_scraper.py $ARGS"
          xvfb-run --auto-servernum --server-args="-screen 0 1280x900x24" \
            python scripts/cricinfo_rich_scraper.py $ARGS

      - name: Create release if needed
        run: |
          gh release view cricinfo-rich 2>/dev/null || \
            gh release create cricinfo-rich \
              --title "Cricinfo Data" \
              --notes "Per-match tables: balls (Hawkeye), match metadata, innings scorecards"
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Build combined parquets
        run: python scripts/combine_cricinfo_parquets.py --cricinfo-dir cricinfo

      - name: Upload results to release
        run: |
          UPLOADED=0
          mkdir -p /tmp/cricinfo_upload
          for dir in t20i_male t20i_female odi_male odi_female test_male test_female; do
            DIR="cricinfo/$dir"
            if [ -d "$DIR" ]; then
              for f in "$DIR"/*.parquet; do
                [ -f "$f" ] || continue
                # Prefix asset name with dir so download can sort by format_gender
                PREFIXED="/tmp/cricinfo_upload/${dir}__$(basename "$f")"
                cp "$f" "$PREFIXED"
                gh release upload cricinfo-rich "$PREFIXED" --clobber 2>/dev/null && UPLOADED=$((UPLOADED + 1)) || true
              done
            fi
          done
          rm -rf /tmp/cricinfo_upload
          # Upload fixtures.parquet (top-level, not in a format subdirectory)
          if [ -f "cricinfo/fixtures.parquet" ]; then
            gh release upload cricinfo-rich "cricinfo/fixtures.parquet" --clobber 2>/dev/null && UPLOADED=$((UPLOADED + 1)) || true
          fi
          # Upload combined parquets for remote R loader access
          for f in cricinfo/combined/cricinfo_*.parquet; do
            [ -f "$f" ] || continue
            gh release upload cricinfo-rich "$f" --clobber 2>/dev/null && UPLOADED=$((UPLOADED + 1)) || true
          done
          echo "Uploaded $UPLOADED parquet files"
          echo "## Cricinfo Scrape Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          TOTAL=$(find cricinfo -name "*.parquet" 2>/dev/null | wc -l)
          echo "- **Total parquet files**: $TOTAL" >> $GITHUB_STEP_SUMMARY
          echo "- **Uploaded this run**: $UPLOADED" >> $GITHUB_STEP_SUMMARY
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Create issue on failure
        if: failure()
        uses: actions/github-script@v7
        with:
          script: |
            const title = `Cricinfo Scrape Failed - ${new Date().toISOString().split('T')[0]}`;
            const body = `## Workflow Failure

            The Cricinfo scrape workflow failed.

            - **Run ID**: ${{ github.run_id }}
            - **Run URL**: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
            - **Triggered by**: ${{ github.event_name }}
            - **Series IDs**: ${{ inputs.series_ids || 'scheduled (auto from CSV)' }}
            - **Format**: ${{ inputs.format || 'all' }}

            ### Common Failure Causes
            - Akamai bot detection (stealth patches may need updating)
            - Playwright/Chromium compatibility issue
            - Xvfb display server failure
            - ESPN Cricinfo site structure change (__NEXT_DATA__ format)

            Please check the workflow logs for details.
            `;

            const issues = await github.rest.issues.listForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              state: 'open',
              labels: 'workflow-failure,cricinfo'
            });

            const today = new Date().toISOString().split('T')[0];
            const existingIssue = issues.data.find(i => i.title.includes(today));

            if (!existingIssue) {
              await github.rest.issues.create({
                owner: context.repo.owner,
                repo: context.repo.repo,
                title: title,
                body: body,
                labels: ['workflow-failure', 'cricinfo']
              });
            }
